from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
from typing import List, Optional, Tuple, Dict, Any

# Dictionnaire pour stocker les mod√®les et tokenizers charg√©s en m√©moire
_loaded_models_cache = {}

# --- Configuration des mod√®les Hugging Face (HF) ---
HF_MODELS_CONFIG = {
    "BART-large": {
        "model_name": "facebook/bart-large-cnn", # Entra√Æn√© sp√©cifiquement pour le r√©sum√©
        "prefix": "" # BART n'a g√©n√©ralement pas besoin de pr√©fixe
    },
    "T5-base": {
        "model_name": "t5-base",
        "prefix": "summarize: " # T5 b√©n√©ficie de ce pr√©fixe
    },
    "Pegasus": {
        "model_name": "google/pegasus-cnn_dailymail", # Un bon Pegasus pour le r√©sum√© g√©n√©ral
        "prefix": "" # Pegasus n'a g√©n√©ralement pas besoin de pr√©fixe
    }
}

# --- Fonction de chargement de mod√®le HF ---
def load_hf_model_and_tokenizer(model_key: str, device: torch.device) -> Tuple[Any, Any]:
    """
    Charge un mod√®le Hugging Face et son tokenizer une seule fois et les met en cache.
    """
    if model_key not in HF_MODELS_CONFIG:
        raise ValueError(f"Mod√®le HF '{model_key}' non configur√©.")

    model_name = HF_MODELS_CONFIG[model_key]["model_name"]

    if model_name in _loaded_models_cache:
        return _loaded_models_cache[model_name]["model"], _loaded_models_cache[model_name]["tokenizer"]

    print(f"   - ‚è≥ Chargement du mod√®le '{model_name}' sur {device}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        model.to(device)
        _loaded_models_cache[model_name] = {"tokenizer": tokenizer, "model": model}
        print(f"   - ‚úÖ Mod√®le '{model_name}' charg√©.")
        return model, tokenizer
    except Exception as e:
        print(f"‚ùå ERREUR lors du chargement de '{model_name}': {e}")
        return None, None

# --- Fonction de r√©sum√© pour un seul texte (HF ou OpenAI) ---
def summarize_text_abstractively(
    text: str,
    model_type: str, # "hf" ou "openai"
    hf_model_key: Optional[str] = None, # Cl√© du mod√®le HF (e.g., "BART-large")
    openai_api_key: Optional[str] = None,
    openai_model_name: str = "gpt-3.5-turbo",
    max_length: int = 150, # Longueur max du r√©sum√© g√©n√©r√©
    min_length: int = 40,  # Longueur min du r√©sum√© g√©n√©r√©
    device: Optional[torch.device] = None
) -> Optional[str]:
    """
    G√©n√®re un r√©sum√© abstractif pour un texte donn√© en utilisant soit un mod√®le Hugging Face,
    soit l'API OpenAI.
    """
    if not text or not text.strip():
        return None

    summary_result = None

    if model_type == "hf":
        if hf_model_key is None or hf_model_key not in HF_MODELS_CONFIG:
            print(f"‚ùå ERREUR : Cl√© de mod√®le HF non sp√©cifi√©e ou non valide pour le type 'hf'.")
            return None
        
        model, tokenizer = load_hf_model_and_tokenizer(hf_model_key, device)
        if model is None or tokenizer is None:
            return None # Erreur de chargement d√©j√† logg√©e

        input_text = HF_MODELS_CONFIG[hf_model_key]["prefix"] + text
        
        # --- CORRECTION DE LA LIGNE SUIVANTE ---
        # Fixe une limite d'entr√©e explicite pour √©viter l'OverflowError
        # La limite typique pour BART est 1024, pour T5-base est 512
        # On peut la rendre dynamique si besoin en fonction du model_name
        fixed_max_input_length = 1024 if "bart" in hf_model_key.lower() else (512 if "t5" in hf_model_key.lower() else 1024)
        inputs = tokenizer(input_text, return_tensors="pt", max_length=fixed_max_input_length, truncation=True)
        # --- FIN DE LA CORRECTION ---

        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        try:
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=max_length,
                min_length=min_length,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True
            )
            summary_result = tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()
        except Exception as e:
            print(f"      - ‚ùå Erreur lors du r√©sum√© HF : {e}")
            summary_result = None

    elif model_type == "openai":
        if openai_api_key is None:
            print("‚ùå ERREUR : Cl√© API OpenAI non fournie pour le type 'openai'.")
            return None
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)

            response = client.chat.completions.create(
                model=openai_model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing technical documents and courses. Provide a concise, clear and comprehensive summary."},
                    {"role": "user", "content": f"Please summarize the following text:\n\n{text}"}
                ],
                max_tokens=max_length,
                temperature=0.7,
            )
            summary_result = response.choices[0].message.content.strip()
        except ImportError:
            print("‚ùå ERREUR : La librairie 'openai' n'est pas install√©e. `pip install openai`.")
            summary_result = None
        except Exception as e:
            print(f"‚ùå ERREUR lors de l'appel √† l'API OpenAI : {e}")
            summary_result = None
    else:
        print(f"‚ùå ERREUR : Type de mod√®le '{model_type}' non support√©.")
        summary_result = None
    
    return summary_result if summary_result else None


# --- Fonction orchestrant le r√©sum√© abstrait d'un document entier (Map-Reduce) ---
def summarize_document_abstractively(
    extracted_chunks: List[str],
    hf_chunk_model_key: str, # Mod√®le HF pour r√©sumer chaque chunk
    final_summary_model_type: str, # "hf" ou "openai"
    final_hf_model_key: Optional[str] = None, # Mod√®le HF pour le r√©sum√© final (si final_summary_model_type est "hf")
    openai_api_key: Optional[str] = None,
    openai_model_name: str = "gpt-3.5-turbo",
    max_chunk_summary_length: int = 100, # Longueur max des r√©sum√©s de chunks
    max_final_summary_length: int = 400, # Longueur max du r√©sum√© final du document
    device: torch.device = None
) -> Optional[str]:
    """
    Orchestre le r√©sum√© abstractif d'un document long en utilisant une strat√©gie Map-Reduce.
    1. R√©sume chaque chunk extrait (Map).
    2. Concat√®ne ces r√©sum√©s de chunks.
    3. R√©sume le texte combin√© pour obtenir le r√©sum√© final du document (Reduce).
    """
    if not extracted_chunks:
        print("‚ùå AVERTISSEMENT : La liste de chunks extraits est vide. Impossible de r√©sumer abstractivement.")
        return None

    print(f"\n--- ü§ñ Lancement du R√©sum√© Abstractif (Map-Reduce) ---")

    # √âtape 1 : R√©sum√© abstractif de chaque chunk (Map)
    abstractive_chunk_summaries = []
    print(f"  - R√©sum√© de chaque chunk avec {hf_chunk_model_key}...")
    for i, chunk_text in enumerate(extracted_chunks):
        print(f"    - Traitement du chunk {i+1}/{len(extracted_chunks)}...")
        chunk_abs_summary = summarize_text_abstractively(
            text=chunk_text,
            model_type="hf",
            hf_model_key=hf_chunk_model_key,
            max_length=max_chunk_summary_length,
            min_length=20,
            device=device
        )
        if chunk_abs_summary:
            abstractive_chunk_summaries.append(chunk_abs_summary)
        else:
            print(f"      - ‚ö†Ô∏è AVERTISSEMENT : R√©sum√© abstractif √©chou√© pour le chunk {i+1}. Ignor√©.")

    if not abstractive_chunk_summaries:
        print("‚ùå Aucun r√©sum√© de chunk abstractif n'a pu √™tre g√©n√©r√©.")
        return None

    # √âtape 2 : Concat√©nation des r√©sum√©s de chunks
    combined_chunk_summaries = "\n\n".join(abstractive_chunk_summaries)
    print(f"  - R√©sum√©s de chunks combin√©s. Longueur totale : {len(combined_chunk_summaries.split())} mots.")

    # √âtape 3 : R√©sum√© final du document (Reduce)
    print(f"  - G√©n√©ration du r√©sum√© final du document avec le mod√®le '{final_summary_model_type}'...")
    final_document_summary = summarize_text_abstractively(
        text=combined_chunk_summaries,
        model_type=final_summary_model_type,
        hf_model_key=final_hf_model_key,
        openai_api_key=openai_api_key,
        openai_model_name=openai_model_name,
        max_length=max_final_summary_length,
        min_length=50,
        device=device
    )

    if final_document_summary:
        print("‚úÖ R√©sum√© Abstractif Global termin√©.")
        return final_document_summary
    else:
        print("‚ùå √âchec de la g√©n√©ration du r√©sum√© abstractif global.")
        return None
