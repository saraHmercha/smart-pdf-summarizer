import fitz
import re
import unicodedata
import html
from collections import Counter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import Optional

def preprocess_text(pdf_path: str) -> Optional[str]:
    print(f"   - Lancement du Pr√©traitement pour le fichier : {pdf_path} ---")
    print(f"   - Extraction du texte brut...")
    try:
        doc = fitz.open(pdf_path)
        raw_text = "".join([page.get_text("text", sort=True) for page in doc])
        doc.close()
        if not raw_text.strip():
            print("‚ùå AVERTISSEMENT : Le PDF ne contient aucun texte extractible.")
            return None
    except Exception as e:
        print(f"‚ùå ERREUR CRITIQUE lors de la lecture du PDF : {e}")
        return None

    # --- NOUVELLE LOGIQUE POUR G√âRER LA SUPPRESSION DES SECTIONS DE FIN ---
    # Convertit les entit√©s HTML, normalise les guillemets et g√®re les coupures de mots
    text = html.unescape(raw_text)
    text = text.replace("‚Äú", '"').replace("‚Äù", '"').replace("‚Äò", "'").replace("‚Äô", "'")
    text = re.sub(r'-\n', '', text) # G√®re les mots coup√©s en fin de ligne

    # Appliquez les split sur le texte brut pour isoler la partie principale du contenu
    # Gardez la partie principale du texte et retirez les sections de fin
    
    # Motif pour les sections de fin courantes (References, Acknowledgements, Appendix)
    # Utilisez re.split et prenez la premi√®re partie, qui est le contenu principal
    main_content_end_pattern = r'\n\s*(References|Acknowledgements?|Appendix|Bibliography)\s*\n'
    match = re.search(main_content_end_pattern, text, flags=re.IGNORECASE)
    if match:
        text = text[:match.start()] # Prend tout le texte AVANT le d√©but de la section trouv√©e

    # Motifs sp√©cifiques de copyright/licence qui peuvent √™tre √† la fin du document
    copyright_pattern = r'The Author\(s\) \d{4}.*'
    license_pattern = r'Open Access This chapter is licensed under the terms of the Creative Commons Attribution-NonCommercial 4\.0 International License.*'
    
    text = re.split(copyright_pattern, text, flags=re.DOTALL)[0] if re.search(copyright_pattern, text, flags=re.DOTALL) else text
    text = re.split(license_pattern, text, flags=re.DOTALL)[0] if re.search(license_pattern, text, flags=re.DOTALL) else text
    # --- FIN NOUVELLE LOGIQUE ---

    # Nettoyage g√©n√©ral du texte (appliqu√© APR√àS la suppression des sections de fin)
    text = re.sub(r'\n\s*\n', '\n\n', text)        # Paragraphes vides multiples en un seul
    text = re.sub(r'[ \t]+', ' ', text)             # Espaces et tabulations multiples
    text = unicodedata.normalize("NFKC", text)      # Normalisation Unicode

    # Suppression des ent√™tes/pieds r√©p√©titifs (peut √™tre plus robuste si fait par page)
    # Cette heuristique est √† garder car elle attrape les ent√™tes qui ne sont pas des sections
    lines = text.split('\n')
    line_counts = Counter(
        line.strip() for line in lines
        if 10 < len(line.strip()) < 100 and not line.strip().isdigit() and not line.strip().isupper()
    )
    frequent_lines = {line for line, count in line_counts.items() if count > 2}

    if frequent_lines:
        print(f"   - Suppression de {len(frequent_lines)} lignes d'en-t√™te/pied de page fr√©quentes.")
        cleaned_text = '\n'.join(line for line in lines if line.strip() not in frequent_lines)
    else:
        cleaned_text = text

    # Nettoyage final des espaces et caract√®res invisibles
    cleaned_text = re.sub(r'\n\s*\n', '\n\n', cleaned_text).strip()
    cleaned_text = re.sub(r'[\u200B-\u200D\uFEFF]', '', cleaned_text)
    
    print("‚úÖ Pr√©traitement termin√© avec succ√®s.")
    return cleaned_text

def filter_irrelevant_chunks(chunks: list[str]) -> list[str]:
    keywords_to_exclude = [
        "creative commons", "license", "springer", "rights reserved", "doi", "chapter",
        "permitted use", "noncommercial", "creativecommons",
        "fig.", "figure", "table", "tableau", "annex", "appendice"
        # "references", "acknowledgment", "acknowledgement", "table of contents"
        # Ces termes sont maintenant g√©r√©s par la suppression des sections de fin,
        # ou ne devraient pas √™tre exclus s'ils apparaissent dans le texte principal comme titres de section.
        # Laissez-les ici si vous voulez filtrer les mentions DANS les chunks.
    ]
    filtered_chunks = [chunk for chunk in chunks if not any(kw in chunk.lower() for kw in keywords_to_exclude)]
    return filtered_chunks

def split_text_into_chunks(text: str, chunk_size: int = 800, chunk_overlap: int = 128) -> list[str]:
    if not text:
        print("‚ùå AVERTISSEMENT : Texte vide fourni pour le d√©coupage.")
        return []

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""]
    )

    chunks = splitter.split_text(text)
    print(f"‚úÖ D√©coupage termin√©. Nombre de chunks bruts cr√©√©s : {len(chunks)}")

    filtered_chunks_by_keywords = filter_irrelevant_chunks(chunks)
    final_filtered_chunks = [chunk for chunk in filtered_chunks_by_keywords if len(chunk.split()) > 50]

    if len(final_filtered_chunks) < len(chunks):
        print(f"üßπ {len(chunks) - len(final_filtered_chunks)} chunks supprim√©s (bruit ou trop courts).")
    if not final_filtered_chunks:
        print("‚ùå AVERTISSEMENT : Aucun chunk pertinent n'a √©t√© conserv√© apr√®s le filtrage.")

    print(f"\n--- Affichage des {min(3, len(final_filtered_chunks))} premiers chunks filtr√©s ---")
    for i, chunk in enumerate(final_filtered_chunks[:3]):
        print(f"\n--- üß© Chunk {i+1} ({len(chunk.split())} mots) ---\n{chunk[:400]}...\n")

    return final_filtered_chunks
